---
title: "2. Word2vec"
date: 2019-07-23
categories: NLP Deep_Learning Word2vec
---
# Word2vec
NLP 모델 중 하나

## 개념
* '주위 단어가 비슷하면 해당 단어의 의미는 유사하다'라는 아이디어.
*	단어, 구, 문장, 단락, 문서의 의미를 알아내는 작업을 함.
*	단어를 계산에 적용할 수 있는 숫자로 변환해서 다음에 올 단어를 예측하는 모델.
*	다음 단어를 예측하기 위해 Word Embedding을 구현해야 하는데, CBOW 또는 Skip-Gram 알고리즘을 사용.
*	기존의 알고리즘에 비해 자연어 처리에서 엄청난 향상을 가져왔다.
*	변환된 벡터가 단순한 수학적 존재 이상의 복잡한 개념 표현을 넘어 추론까지도 쉽게 구현할 수 있다는 점에서 대단한 의미를 가짐.
*	단어를 트레이닝 시킬 때 주위 단어를 label로 매치하여 최적화
*	단어를 의미를 내포한 dense vector로 매칭하는 것

## 종류

### CBOW(Continuous Bag-of-Words)
*	컨텍스트로부터 찾고자 하는 목표 단어를 예측하는 모델을 말함.
*	주어진 단어에 대해 앞 뒤로 N/2개 씩 총 N개의 단어를 입력으로 사용하여, 주어진 단어를 맞추기 위한 네트워크를 만듦.
*	크기가 작은 데이터셋에 적합.
  * 예시
    * 1. __ 가 맛있다.
    * 2. __ 를 타는 것이 재미있다.
    * 3. 평소보다 두 __ 로 많이 먹어서 __ 가 아프다.

### Skip-Gram
*	처리하려고 하는 현재의 단어 하나를 사용해서 주변 단어들의 발생을 유추하는 모델.
*	예측하는 단어들은 현재 단어 주변에서 샘플링.
*	‘가까이 있는 단어일수록 현재 단어와 관련이 더 많다’는 원리를 적용하기 위해 멀리 떨어진 단어를 낮은 확률로 선택하는 방법을 사용.
*	나머지는 CBOW 모델과 방향만 반대이고 거의 비슷함.
*	Skip-Gram 모델은 크기가 큰 데이터셋에 적합하여, 최근일수록 더욱 많은 데이터를 갖고 있기 때문에 주로 Skip-Gram 모델 사용.
  *	예시: 특정 단어 주변에 올 수 있는 단어 예측
    *	1. * 배* 가 맛있다.
    *	2. * 배*를 타는 것이 재미있다.
    *	3. 평소보다 두 * 배*로 많이 먹어서 * 배*가 아프다.

## 용어정리

### Word Embedding
*	고차원의 데이터를 그보다 낮은 차원으로 변환하면서 모든 데이터 간의 관계가 성립되도록 처리하는 과정.
*	컴퓨터가 문자나 그림 등을 인식하게 하기위해 데이터를 벡터(숫자)로 변환하는 것.
*	단어 자체를 아스키코드나 유니코드로 처리해서 사용해왔지만, 추론을 할 수 없기 때문에, 숫자로 변환할 필요가 있음.
*	Word Embedding 초기 모델에는 NNLM과 RNNLM이 있음.
*	최근에는 CBOW와 Skip-Gram 모델을 많이 사용하여 학습.
*	Word Embedding 프로세스에는 신경망, 차원감소, 확률적 모델, 문맥상 표현 등의 여러 가지 처리과정들이 포함됨
